{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3609a696-ccfe-4c83-bd1c-9237b13f0a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Job Pipeline â€“ Task level parameter\n",
    "\n",
    "This example demonstrates:\n",
    "- How to **set a parameter value** in one notebook using `dbutils.jobs.taskValues.set()`\n",
    "- How to **retrieve the value** in another notebook using `dbutils.jobs.taskValues.get()`\n",
    "- Use case: We filter transaction data (`txns`) by `state` and `city` parameters passed between notebooks.\n",
    "\n",
    "**Dataset Fields:**\n",
    "`txnid, txndate, custid, amount, prodcategory, prodname, city, state, paymenttype`\n",
    "\n",
    "**Flow:**\n",
    "1. The parent notebook defines parameters (`state`, `city`)  \n",
    "2. Reads sample `txns` data  \n",
    "3. Filters data and sets a **record count** value using `dbutils.jobs.taskValues.set()`  \n",
    "4. The child notebook retrieves this value and performs further logic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6840d267-e67a-4757-bd81-3ce758c0ad00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "txns_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/Volumes/inceptez_catalog/inputdb/customerdata/txns\") \\\n",
    "\t.toDF(\"txnid\",\"txndate\",\"custid\",\"amount\",\"prodcategory\",\"prodname\",\"city\",\"state\",\"paymenttype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf3b7bf-7727-4b19-9ef8-cdd2751ec464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state = \"Texas\"\n",
    "city = \"Dallas\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "filtered_df = txns_df.filter((txns_df.state == state) & (txns_df.city == city))\n",
    "df1 = filtered_df.withColumn(\"created_ts\",F.current_timestamp())\n",
    "df1.write.format(\"delta\").mode(\"append\").saveAsTable(\"inceptez_catalog.outputdb.tbltxnsjob\")\n",
    "print(\"data written into table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "436dc016-c512-40f1-9c04-19b66a1efde3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store the record_count value to be used in another notebook\n",
    "dbutils.jobs.taskValues.set(\n",
    "    key=\"txn_record_count\",\n",
    "    value=record_count\n",
    ")\n",
    "\n",
    "print(\"Record count stored successfully in job task values.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab08-taskparam-parent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
